{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers datasets tokenizers accelerate huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install transformers==4.35.0\n",
    "!pip install datasets==2.14.0\n",
    "!pip install accelerate==0.24.0\n",
    "!pip install wandb\n",
    "!pip install tiktoken\n",
    "!pip install matplotlib pandas numpy\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade datasets to latest version\n",
    "!pip install datasets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "from datasets import load_dataset\n",
    "tinygsm_dataset = load_dataset(\"TinyGSM/TinyGSM\")\n",
    "gsm8k_dataset = load_dataset(\"gsm8k\", \"main\")  # for evaluation\n",
    "\n",
    "print(f\"TinyGSM train size: {len(tinygsm_dataset['train'])}\")\n",
    "print(f\"GSM8k train size {len(gsm8k_dataset['train'])}\")\n",
    "print(f\"GSM8k test size {len(gsm8k_dataset['test'])}\")\n",
    "\n",
    "print(\"\\nTinyGSM example:\")\n",
    "print(tinygsm_dataset['train'][0])\n",
    "\n",
    "print(\"\\nGSM8k example:\")\n",
    "print(gsm8k_dataset['train'][0])\n",
    "\n",
    "print(\"Keys available in TinyGSM dataset example:\", tinygsm_dataset['train'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import tiktoken\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_from_code(code_str: str):\n",
    "    local_vars = {}\n",
    "    try:\n",
    "        # Execute the code string which defines a function\n",
    "        exec(code_str, globals(), local_vars)\n",
    "        import re\n",
    "        function_name_match = re.search(r'def\\s+(\\w+)\\s*\\(', code_str)\n",
    "        if function_name_match:\n",
    "            function_name = function_name_match.group(1)\n",
    "        else:\n",
    "            return \"Execution Failed: Function name not found.\"\n",
    "        result_func = local_vars[function_name]\n",
    "        final_result = result_func()\n",
    "        return final_result\n",
    "    except Exception as e:\n",
    "        return f\"Code Execution Error: {e}\"\n",
    "\n",
    "# Visualize a few examples\n",
    "for i in range(3):\n",
    "    ex = tinygsm_dataset['train'][i]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Question: {ex['question']}\")\n",
    "    code_str = ex['code']\n",
    "    print(f\"Code:\\n{code_str}\")\n",
    "    answer = get_result_from_code(code_str)\n",
    "    print(f\"Calculated Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50257\n",
    "N_EMBD = 384       # reduce for ~80M params\n",
    "N_LAYER = 12\n",
    "N_HEAD = 8\n",
    "SEQ_LEN = 128\n",
    "\n",
    "DATA_DIR = Path(\"./tinygsm_data\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR = Path(\"./tokenized_cache\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "GRAD_ACCUM_STEPS = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.1\n",
    "WARMUP_STEPS = 500\n",
    "MAX_STEPS = 20000\n",
    "EVAL_INTERVAL = 500\n",
    "SEED = 42\n",
    "CHECKPOINT_DIR = Path(\"./checkpoints\")\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "NUM_WORKERS = 4\n",
    "NUM_PROC = 11\n",
    "BATCH_SIZE = 25000\n",
    "WRITER_BATCH_SIZE = 10000\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(\"tiktoken vocab size (n_vocab):\", getattr(enc, \"n_vocab\", VOCAB_SIZE))\n",
    "\n",
    "def format_io(question: str, answer: str):\n",
    "    input_pref = f\"Question: {question}\\nAnswer:\"\n",
    "    answer_text = str(answer).strip()\n",
    "    return input_pref, answer_text\n",
    "\n",
    "def tokenize_example(example):\n",
    "    try:\n",
    "        q_text, a_text = format_io(example['question'], example['code'])\n",
    "        q_ids = enc.encode(q_text)\n",
    "        a_ids = enc.encode(a_text)\n",
    "        ids = q_ids + a_ids\n",
    "\n",
    "        labels = [-100] * len(q_ids) + a_ids\n",
    "        return {\"ids\": ids, \"labels\": labels, \"len\": len(ids)}\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping example due to error: {e}\")\n",
    "        return {\"ids\": [], \"labels\": [], \"len\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tinygsm_dataset['train'][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(f\"Available CPU cores: {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "def batch_tokenize_example(batch):\n",
    "    \"\"\"Optimized tokenization function\"\"\"\n",
    "    ids_list = []\n",
    "    labels_list = []\n",
    "    len_list = []\n",
    "\n",
    "    for question, code in zip(batch[\"question\"], batch[\"code\"]):\n",
    "        q_text, a_text = format_io(question, code)\n",
    "        q_ids = enc.encode(q_text)\n",
    "        a_ids = enc.encode(a_text)\n",
    "\n",
    "        ids = q_ids + a_ids\n",
    "        labels = [-100] * len(q_ids) + a_ids\n",
    "\n",
    "        ids_list.append(ids)\n",
    "        labels_list.append(labels)\n",
    "        len_list.append(len(ids))\n",
    "\n",
    "    return {\"ids\": ids_list, \"labels\": labels_list, \"len\": len_list}\n",
    "\n",
    "\n",
    "def get_or_create_tokenized_dataset(dataset, split=\"train\", force_retokenize=False):\n",
    "    \"\"\"\n",
    "    Load cached tokenized dataset or create new one.\n",
    "    This ensures you only tokenize once!\n",
    "    \"\"\"\n",
    "    cache_path = CACHE_DIR / f\"{split}_tokenized\"\n",
    "\n",
    "    if cache_path.exists() and not force_retokenize:\n",
    "        print(f\" Loading cached tokenized dataset from {cache_path}\")\n",
    "        tokenized = load_from_disk(str(cache_path))\n",
    "        print(f\" Loaded {len(tokenized)} examples from cache\")\n",
    "        return tokenized\n",
    "\n",
    "    print(f\"Tokenizing {split} dataset (this will be cached)...\")\n",
    "    tokenized = dataset.map(\n",
    "        batch_tokenize_example,\n",
    "        remove_columns=dataset.column_names,\n",
    "        batched=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_proc=NUM_PROC,\n",
    "        desc=f\"tokenizing {split}\",\n",
    "        writer_batch_size=WRITER_BATCH_SIZE,\n",
    "        load_from_cache_file=True\n",
    "    )\n",
    "\n",
    "    # Save to cache\n",
    "    print(f\"Saving tokenized dataset to cache: {cache_path}\")\n",
    "    tokenized.save_to_disk(str(cache_path))\n",
    "    print(f\"Cached tokenized dataset for future use!\")\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def build_memmap_fast(dataset_dict, splits=(\"train\",), overwrite=False, use_cache=True):\n",
    "    \"\"\"\n",
    "    Fast memmap builder with caching support.\n",
    "    Only tokenizes once, reuses cached data on subsequent runs.\n",
    "    \"\"\"\n",
    "    for split in splits:\n",
    "        out_ids = DATA_DIR / f\"{split}_ids.bin\"\n",
    "        out_labels = DATA_DIR / f\"{split}_labels.bin\"\n",
    "        meta_json = DATA_DIR / f\"{split}_meta.json\"\n",
    "\n",
    "        # Check if memmap already exists\n",
    "        if out_ids.exists() and out_labels.exists() and not overwrite:\n",
    "            print(f\"{split} memmap already exists, skipping (use overwrite=True to rebuild).\")\n",
    "\n",
    "            # Load and print stats if metadata exists\n",
    "            if meta_json.exists():\n",
    "                with open(meta_json, \"r\") as f:\n",
    "                    meta = json.load(f)\n",
    "                    print(f\"  Total tokens: {meta['total_len']:,}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing split: {split}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        ds = dataset_dict[split]\n",
    "        print(f\"Dataset size: {len(ds):,} examples\")\n",
    "\n",
    "        # Use cached tokenization or create new\n",
    "        tokenized = get_or_create_tokenized_dataset(\n",
    "            ds,\n",
    "            split=split,\n",
    "            force_retokenize=not use_cache\n",
    "        )\n",
    "\n",
    "        # Calculate total length\n",
    "        print(\"Calculating total token count...\")\n",
    "        total_len = int(np.sum(np.asarray(tokenized[\"len\"]), dtype=np.uint64))\n",
    "        print(f\" Total tokens for {split}: {total_len:,}\")\n",
    "\n",
    "        if total_len <= 0:\n",
    "            print(f\"⚠ No data to write for split {split}. Skipping memmap creation.\")\n",
    "            continue\n",
    "\n",
    "        # Data types for memmap\n",
    "        ids_dtype = np.uint16\n",
    "        lbls_dtype = np.int32\n",
    "\n",
    "        # Calculate expected file sizes\n",
    "        ids_size_gb = (total_len * np.dtype(ids_dtype).itemsize) / (1024**3)\n",
    "        lbls_size_gb = (total_len * np.dtype(lbls_dtype).itemsize) / (1024**3)\n",
    "        print(f\"Expected file sizes:\")\n",
    "        print(f\"  - IDs: {ids_size_gb:.2f} GB\")\n",
    "        print(f\"  - Labels: {lbls_size_gb:.2f} GB\")\n",
    "        print(f\"  - Total: {ids_size_gb + lbls_size_gb:.2f} GB\")\n",
    "\n",
    "        # Create memmaps\n",
    "        print(f\"\\nCreating memmap files...\")\n",
    "        out_ids.parent.mkdir(parents=True, exist_ok=True)\n",
    "        ids_arr = np.memmap(out_ids, dtype=ids_dtype, mode=\"w+\", shape=(total_len,))\n",
    "        labels_arr = np.memmap(out_labels, dtype=lbls_dtype, mode=\"w+\", shape=(total_len,))\n",
    "\n",
    "        # Write data efficiently - iterate through dataset batches\n",
    "        print(f\"Writing {total_len:,} tokens to disk...\")\n",
    "        idx = 0\n",
    "        batch_size = 100000  # Process 100k examples at a time\n",
    "\n",
    "        for i in range(0, len(tokenized), batch_size):\n",
    "            batch_end = min(i + batch_size, len(tokenized))\n",
    "            batch = tokenized.select(range(i, batch_end)).with_format(\"numpy\")\n",
    "\n",
    "            # Write each example in the batch\n",
    "            for j in range(len(batch)):\n",
    "                example_ids = batch[\"ids\"][j]\n",
    "                example_labels = batch[\"labels\"][j]\n",
    "                n = len(example_ids)\n",
    "\n",
    "                ids_arr[idx:idx + n] = example_ids\n",
    "                labels_arr[idx:idx + n] = example_labels\n",
    "                idx += n\n",
    "\n",
    "            # Progress update every batch\n",
    "            progress = 100 * idx / total_len\n",
    "            print(f\"  Progress: {idx:,}/{total_len:,} tokens ({progress:.1f}%) - Processed {batch_end:,}/{len(tokenized):,} examples\")\n",
    "\n",
    "            # Flush periodically\n",
    "            if i % (batch_size * 5) == 0:\n",
    "                ids_arr.flush()\n",
    "                labels_arr.flush()\n",
    "\n",
    "        # Final flush\n",
    "        ids_arr.flush()\n",
    "        labels_arr.flush()\n",
    "        print(f\" Finished writing {idx:,} tokens\")\n",
    "\n",
    "        # Save metadata\n",
    "        meta = {\"total_len\": total_len}\n",
    "        with open(meta_json, \"w\") as f:\n",
    "            json.dump(meta, f)\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\" SUCCESS! Memmap files created for {split}\")\n",
    "        print(f\"  - IDs: {out_ids}\")\n",
    "        print(f\"  - Labels: {out_labels}\")\n",
    "        print(f\"  - Metadata: {meta_json}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "build_memmap_fast(\n",
    "    tinygsm_dataset,\n",
    "    splits=(\"train\",),\n",
    "    overwrite=True,  # Set True to rebuild memmaps\n",
    "    use_cache=True    # Set False to force re-tokenization\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPLETE! Your data is ready for training.\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext runs will be MUCH faster because tokenization is cached!\")\n",
    "print(\"To force re-tokenization: build_memmap_fast(..., use_cache=False)\")\n",
    "print(\"To rebuild memmaps: build_memmap_fast(..., overwrite=True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"GPU Available:\" if torch.cuda.is_available() else \"GPU Not Available - Check back later\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                       .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None,\n",
    "                                              dropout_p=self.attn_dropout.p if self.training else 0.0,\n",
    "                                              is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 512\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = True\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop=nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)),\n",
    "                                 targets.view(-1), ignore_index=-100)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MemmapDataset(Dataset):\n",
    "    def __init__(self, ids_path, labels_path, seq_length=512):\n",
    "        self.ids = np.memmap(ids_path, dtype=np.uint16, mode='r')\n",
    "        self.labels = np.memmap(labels_path, dtype=np.int32, mode='r')\n",
    "        self.seq_length = seq_length\n",
    "        self.total_length = len(self.ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.total_length - self.seq_length) // self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.seq_length\n",
    "        end = start + self.seq_length\n",
    "\n",
    "        ids = torch.from_numpy(self.ids[start:end].astype(np.int64))\n",
    "        labels = torch.from_numpy(self.labels[start:end].astype(np.int64))\n",
    "\n",
    "        return ids, labels\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = MemmapDataset(\n",
    "    DATA_DIR / \"train_ids.bin\",\n",
    "    DATA_DIR / \"train_labels.bin\",\n",
    "    seq_length=512  # Adjust based on model's context length\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,  # Adjust based on GPU memory\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset ready: {len(dataset):,} sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_DIR = Path(\"/content/drive/MyDrive/tinygsm_data\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss_tinygsm(model, get_batch, eval_iters=200, ctx=torch.no_grad()):\n",
    "    losses = {}\n",
    "    model.eval()\n",
    "    with ctx:\n",
    "        for split in ['train', 'val']:\n",
    "            loss_vals = torch.zeros(eval_iters)\n",
    "            for i in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                logits, loss = model(X, Y)\n",
    "                loss_vals[i] = loss.item()\n",
    "            losses[split] = loss_vals.mean().item()\n",
    "    model.train()\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    # Model\n",
    "    block_size: int = 512\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = True\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 32\n",
    "    learning_rate: float = 6e-4\n",
    "    max_iters: int = 50000\n",
    "    weight_decay: float = 0.1\n",
    "    beta1: float = 0.9\n",
    "    beta2: float = 0.95\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # Learning rate schedule\n",
    "    warmup_steps: int = 2000\n",
    "    min_lr: float = 6e-5\n",
    "\n",
    "    # Evaluation\n",
    "    eval_interval: int = 500\n",
    "    eval_iters: int = 200\n",
    "\n",
    "    # Logging\n",
    "    log_interval: int = 10\n",
    "\n",
    "    # Checkpointing\n",
    "    checkpoint_dir: str = \"/content/drive/MyDrive/tinygsm_checkpoints\"\n",
    "    save_interval: int = 5000\n",
    "\n",
    "    # Data\n",
    "    data_dir: str = \"/content/drive/MyDrive/tinygsm_data\"\n",
    "\n",
    "    # System\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dtype: str = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "    compile: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "def compute_loss_and_backprop(model, X, Y, optimizer, scaler=None):\n",
    "    model.train()\n",
    "    X = X.to(DEVICE)\n",
    "    Y = Y.to(DEVICE)\n",
    "    logits = model(X)\n",
    "    B, T, V = logits.shape\n",
    "    loss = loss_fn(logits.view(B*T, V), Y.view(B*T))\n",
    "    if scaler is not None:\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    else:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /content/drive/MyDrive/tinygsm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /teamspace/studios/this_studio/tinygsm_data/* /content/drive/MyDrive/tinygsm_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_DIR = Path(\"/teamspace/studios/this_studio/tinygsm_data\")\n",
    "\n",
    "print(\"train_ids.bin exists:\", (DATA_DIR/\"train_ids.bin\").exists())\n",
    "print(\"train_labels.bin exists:\", (DATA_DIR/\"train_labels.bin\").exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "\n",
    "DATA_DIR = Path(\"/teamspace/studios/this_studio/tinygsm_data\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"QUICK VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check GPU\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "# Check data files\n",
    "print(f\"\\nData directory: {DATA_DIR}\")\n",
    "for file in [\"train_ids.bin\", \"train_labels.bin\"]:\n",
    "    path = DATA_DIR / file\n",
    "    if path.exists():\n",
    "        size_mb = os.path.getsize(path) / (1024**2)\n",
    "        print(f\" {file}: {size_mb:.2f} MB\")\n",
    "    else:\n",
    "        print(f\" {file}: NOT FOUND\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "from contextlib import nullcontext\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_lr(it, config):\n",
    "    \"\"\"Learning rate schedule with warmup and cosine decay\"\"\"\n",
    "    if it < config.warmup_iters:\n",
    "        return config.learning_rate * it / config.warmup_iters\n",
    "    if it > config.lr_decay_iters:\n",
    "        return config.min_lr\n",
    "    decay_ratio = (it - config.warmup_iters) / (config.lr_decay_iters - config.warmup_iters)\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return config.min_lr + coeff * (config.learning_rate - config.min_lr)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_loader, val_loader, config, ctx, eval_iters=50):\n",
    "    \"\"\"Estimate loss on train and validation sets\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "        if loader is None:\n",
    "            continue\n",
    "        losses = []\n",
    "        data_iter = iter(loader)\n",
    "        for _ in range(min(eval_iters, len(loader))):\n",
    "            try:\n",
    "                X, Y = next(data_iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            X, Y = X.to(config.device), Y.to(config.device)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = sum(losses) / len(losses) if losses else 0.0\n",
    "\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "def train(resume_from=None):\n",
    "    \"\"\"Main training function - matches TinyStories pattern\"\"\"\n",
    "    config = TrainingConfig()\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"TinyGSM Training - Full Dataset Mode\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Model: {config.n_layer} layers, {config.n_head} heads, {config.n_embd} embd\")\n",
    "    print(f\"Block size: {config.block_size}\")\n",
    "    print(f\"Batch size: {config.batch_size} × {config.gradient_accumulation_steps} = {config.batch_size * config.gradient_accumulation_steps} effective\")\n",
    "    print(f\"Learning rate: {config.learning_rate} → {config.min_lr}\")\n",
    "    print(f\"Max iterations: {config.max_iters:,}\")\n",
    "\n",
    "    # Calculate total tokens\n",
    "    tokens_per_iter = config.batch_size * config.gradient_accumulation_steps * config.block_size\n",
    "    total_tokens = tokens_per_iter * config.max_iters\n",
    "    print(f\"Total tokens to process: {total_tokens:,} ({total_tokens/1e9:.2f}B)\")\n",
    "    print(f\"Data directory: {config.data_dir}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Device setup\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "        print(f\" Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print(\" Using CPU\")\n",
    "    config.device = device\n",
    "\n",
    "    # Precision setup\n",
    "    dtype_map = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}\n",
    "    ptdtype = dtype_map.get(config.dtype, torch.float32)\n",
    "    ctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type='cuda', dtype=ptdtype)\n",
    "    use_amp = (config.dtype == 'float16') and (device == 'cuda')\n",
    "\n",
    "    # Load datasets\n",
    "    print(\"\\nLoading datasets...\")\n",
    "    DATA_DIR = Path(config.data_dir)\n",
    "\n",
    "    if not DATA_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Data directory not found: {DATA_DIR}\")\n",
    "\n",
    "    train_dataset = MemmapDataset(\n",
    "        DATA_DIR / \"train_ids.bin\",\n",
    "        DATA_DIR / \"train_labels.bin\",\n",
    "        seq_length=config.block_size\n",
    "    )\n",
    "\n",
    "    # Check if validation exists\n",
    "    val_loader = None\n",
    "    if (DATA_DIR / \"val_ids.bin\").exists():\n",
    "        val_dataset = MemmapDataset(\n",
    "            DATA_DIR / \"val_ids.bin\",\n",
    "            DATA_DIR / \"val_labels.bin\",\n",
    "            seq_length=config.block_size\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=(device == 'cuda')\n",
    "        )\n",
    "        print(f\" Validation dataset: {len(val_dataset):,} sequences\")\n",
    "\n",
    "    # Training dataloader with proper generator\n",
    "    if device == 'cuda':\n",
    "        g = torch.Generator(device='cuda')\n",
    "        g.manual_seed(42)\n",
    "    else:\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(42)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        generator=g,\n",
    "        pin_memory=(device == 'cuda')\n",
    "    )\n",
    "\n",
    "    print(f\" Training dataset: {len(train_dataset):,} sequences\")\n",
    "    print(f\" Training batches per epoch: {len(train_loader):,}\")\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"\\nInitializing model...\")\n",
    "    model_config = GPTConfig(\n",
    "        block_size=config.block_size,\n",
    "        vocab_size=config.vocab_size,\n",
    "        n_layer=config.n_layer,\n",
    "        n_head=config.n_head,\n",
    "        n_embd=config.n_embd,\n",
    "        dropout=config.dropout,\n",
    "        bias=config.bias\n",
    "    )\n",
    "\n",
    "    model = GPT(model_config).to(device)\n",
    "    model.train()\n",
    "    print(f\" Model parameters: {model.get_num_params()/1e6:.2f}M\")\n",
    "\n",
    "    # Compile model\n",
    "    if config.compile and hasattr(torch, 'compile'):\n",
    "        try:\n",
    "            print(\"Compiling model...\")\n",
    "            model = torch.compile(model)\n",
    "            print(\" Model compiled\")\n",
    "        except Exception as e:\n",
    "            print(f\" Compile failed: {e}\")\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        betas=(config.beta1, config.beta2),\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "\n",
    "    # Gradient scaler\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=use_amp)\n",
    "    if use_amp:\n",
    "        print(\" Mixed precision training enabled\")\n",
    "\n",
    "    # Resume from checkpoint\n",
    "    iter_num = 0\n",
    "    best_val_loss = float('inf')\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "\n",
    "    if resume_from:\n",
    "        checkpoint_path = Path(config.checkpoint_dir) / resume_from\n",
    "        if checkpoint_path.exists():\n",
    "            print(f\"\\nLoading checkpoint: {checkpoint_path}\")\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            iter_num = checkpoint.get('iter_num', 0)\n",
    "            best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "            train_loss_list = checkpoint.get('train_loss_list', [])\n",
    "            val_loss_list = checkpoint.get('val_loss_list', [])\n",
    "            print(f\" Resumed from iteration {iter_num}\")\n",
    "\n",
    "    # Training loop\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Starting Training\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    running_loss = 0.0\n",
    "    local_iter_num = 0\n",
    "    epoch = 0\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    try:\n",
    "        # Main iteration loop\n",
    "        pbar = tqdm(total=config.max_iters, initial=iter_num, desc=\"Training\")\n",
    "\n",
    "        while iter_num < config.max_iters:\n",
    "            # Iterate through entire dataset each epoch\n",
    "            for X, Y in train_loader:\n",
    "                if iter_num >= config.max_iters:\n",
    "                    break\n",
    "\n",
    "                # Update learning rate\n",
    "                lr = get_lr(iter_num, config)\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "\n",
    "                # Evaluation\n",
    "                if iter_num % config.eval_interval == 0 and iter_num > 0:\n",
    "                    losses = estimate_loss(model, train_loader, val_loader, config, ctx)\n",
    "\n",
    "                    pbar.write(f\"\\n{'='*70}\")\n",
    "                    pbar.write(f\"Iteration {iter_num}/{config.max_iters} ({100*iter_num/config.max_iters:.1f}%)\")\n",
    "                    pbar.write(f\"Train loss: {losses['train']:.4f}\", end=\"\")\n",
    "                    if 'val' in losses:\n",
    "                        pbar.write(f\" | Val loss: {losses['val']:.4f}\")\n",
    "                        val_loss_list.append(losses['val'])\n",
    "\n",
    "                        # Save best model\n",
    "                        if losses['val'] < best_val_loss:\n",
    "                            best_val_loss = losses['val']\n",
    "                            best_path = Path(config.checkpoint_dir) / 'best_model.pt'\n",
    "                            best_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                            torch.save({\n",
    "                                'model': model.state_dict(),\n",
    "                                'optimizer': optimizer.state_dict(),\n",
    "                                'iter_num': iter_num,\n",
    "                                'best_val_loss': best_val_loss,\n",
    "                                'train_loss_list': train_loss_list,\n",
    "                                'val_loss_list': val_loss_list,\n",
    "                                'config': config,\n",
    "                            }, best_path)\n",
    "                            pbar.write(f\" New best validation loss: {best_val_loss:.4f}\")\n",
    "                    else:\n",
    "                        pbar.write(\"\")\n",
    "\n",
    "                    pbar.write(f\"Learning rate: {lr:.2e}\")\n",
    "                    train_loss_list.append(losses['train'])\n",
    "\n",
    "                    elapsed = (time.time() - t0) / 3600\n",
    "                    remaining = (elapsed / max(1, iter_num)) * (config.max_iters - iter_num)\n",
    "                    pbar.write(f\"Time: {elapsed:.2f}h elapsed | ~{remaining:.2f}h remaining\")\n",
    "                    pbar.write(f\"{'='*70}\")\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "                # Save checkpoint\n",
    "                if iter_num % config.save_interval == 0 and iter_num > 0:\n",
    "                    ckpt_dir = Path(config.checkpoint_dir)\n",
    "                    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                    checkpoint = {\n",
    "                        'model': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'iter_num': iter_num,\n",
    "                        'best_val_loss': best_val_loss,\n",
    "                        'train_loss_list': train_loss_list,\n",
    "                        'val_loss_list': val_loss_list,\n",
    "                        'config': config,\n",
    "                    }\n",
    "                    torch.save(checkpoint, ckpt_dir / f'ckpt_iter_{iter_num}.pt')\n",
    "                    torch.save(checkpoint, ckpt_dir / 'ckpt_latest.pt')\n",
    "                    pbar.write(f\" Checkpoint saved at iteration {iter_num}\")\n",
    "\n",
    "                # Training step with gradient accumulation\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                for micro_step in range(config.gradient_accumulation_steps):\n",
    "                    X_batch = X.to(device, non_blocking=True)\n",
    "                    Y_batch = Y.to(device, non_blocking=True)\n",
    "\n",
    "                    with ctx:\n",
    "                        logits, loss = model(X_batch, Y_batch)\n",
    "                        loss = loss / config.gradient_accumulation_steps\n",
    "\n",
    "                    scaler.scale(loss).backward()\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "                # Gradient clipping\n",
    "                if config.grad_clip != 0.0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "\n",
    "                # Optimizer step\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                # Logging\n",
    "                if iter_num % config.log_interval == 0:\n",
    "                    lossf = running_loss * config.gradient_accumulation_steps\n",
    "                    pbar.set_postfix({'loss': f'{lossf:.4f}', 'lr': f'{lr:.2e}'})\n",
    "                    running_loss = 0.0\n",
    "\n",
    "                iter_num += 1\n",
    "                local_iter_num += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "            epoch += 1\n",
    "            pbar.write(f\" Completed epoch {epoch}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n Training interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "    # Save final checkpoint\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    ckpt_dir = Path(config.checkpoint_dir)\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    final_checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'iter_num': iter_num,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'train_loss_list': train_loss_list,\n",
    "        'val_loss_list': val_loss_list,\n",
    "        'config': config,\n",
    "    }\n",
    "    torch.save(final_checkpoint, ckpt_dir / 'ckpt_final.pt')\n",
    "\n",
    "    print(f\" Final checkpoint saved\")\n",
    "    print(f\" Total iterations: {iter_num:,}\")\n",
    "    print(f\" Total epochs: {epoch}\")\n",
    "    if val_loss_list:\n",
    "        print(f\" Best validation loss: {best_val_loss:.4f}\")\n",
    "    if train_loss_list:\n",
    "        print(f\" Final training loss: {train_loss_list[-1]:.4f}\")\n",
    "\n",
    "    total_time = (time.time() - t0) / 3600\n",
    "    print(f\" Total training time: {total_time:.2f} hours\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    return {\n",
    "        'train_loss_list': train_loss_list,\n",
    "        'val_loss_list': val_loss_list,\n",
    "        'best_val_loss': best_val_loss\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
